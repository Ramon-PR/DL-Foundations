{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dd2f359",
   "metadata": {},
   "source": [
    "# Datasets similar to MNIST "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c0a78",
   "metadata": {},
   "source": [
    "## Classes for microphones and MeshRIR\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94379299",
   "metadata": {},
   "source": [
    "Check pos_src file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32b18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (32, 3)\n",
      "Data type: float64\n",
      "Fortran order: False\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "file_path = 'data/MeshRIR/raw/S32-M441_npy/pos_src.npy'  # Replace with your actual file path\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    version = np.lib.format.read_magic(f)\n",
    "    shape, fortran_order, dtype = np.lib.format._read_array_header(f, version)\n",
    "\n",
    "print(\"Array shape:\", shape)\n",
    "print(\"Data type:\", dtype)\n",
    "print(\"Fortran order:\", fortran_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92f0b37",
   "metadata": {},
   "source": [
    "Check pos_mic file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11f453cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (441, 3)\n",
      "Data type: float64\n",
      "Fortran order: False\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Read only the header of the .npy file\n",
    "file_path = 'data/MeshRIR/raw/S32-M441_npy/pos_mic.npy'  # Replace with your actual file path\n",
    "\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    version = np.lib.format.read_magic(f)\n",
    "    shape, fortran_order, dtype = np.lib.format._read_array_header(f, version)\n",
    "\n",
    "print(\"Array shape:\", shape)\n",
    "print(\"Data type:\", dtype)\n",
    "print(\"Fortran order:\", fortran_order)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2407ca",
   "metadata": {},
   "source": [
    "Check ir file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ede8890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Array shape: (1, 32768)\n",
      "Data type: float64\n",
      "Fortran order: False\n"
     ]
    }
   ],
   "source": [
    "# file_path = f'data/MeshRIR/raw/S32-M441_npy/ir_{0}.npy'  # Replace with your actual file path\n",
    "file_path = f'data/MeshRIR/raw/S1-M3969_npy/ir_{0}.npy'  # Replace with your actual file path\n",
    "\n",
    "with open(file_path, 'rb') as f:\n",
    "    version = np.lib.format.read_magic(f)\n",
    "    shape, fortran_order, dtype = np.lib.format._read_array_header(f, version)\n",
    "\n",
    "print(\"Array shape:\", shape)\n",
    "print(\"Data type:\", dtype)\n",
    "print(\"Fortran order:\", fortran_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f688c461",
   "metadata": {},
   "source": [
    "Implementation of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "411a99dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 meter in : 2.941176470588235 ms\n",
      "Samples per ms: 48.0\n",
      "\n",
      "Mic1 and Mic2 with 1 meter difference in the direction of the source: \n",
      "Are (fs / cs) = 141.1764705882353 samples apart\n"
     ]
    }
   ],
   "source": [
    "# 48000 Hz = 1/s\n",
    "# c = 340 m/s \n",
    "# 1 m -> 1/340 s\n",
    "\n",
    "print(f\"1 meter in : {1/340*1000} ms\")\n",
    "print(f\"Samples per ms: {48000/1000}\")\n",
    "print()\n",
    "print(f\"Mic1 and Mic2 with 1 meter difference in the direction of the source: \\nAre (fs / cs) = {1/340*48000} samples apart\")\n",
    "\n",
    "# samples/s * 1/(m/s) = samples/m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb08b660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1-M3969_npy\n"
     ]
    }
   ],
   "source": [
    "# export\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.utils import check_integrity, download_and_extract_archive, extract_archive, verify_str_arg\n",
    "from urllib.error import URLError\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" \n",
    "    01. Instead of loading numpy files just to check it's shape, I can just read the header (read_npy_header)\n",
    "    02. MNIST uses \"class attributes\" (common to all instances) mirrors and resources to indicate where to download the data\n",
    "        * 02.1 The other attributes are instance attributes, so they are different for each instance\n",
    "    03. From MNIST I also use how it checks for the data, downloads, extracts and the folder tree where it downloads (/(user_def_data_path)/(class_name)/raw)\n",
    "    04. I let the user give the dataset name, so for example, \"S\" is common to both datasets and will download both\n",
    "    05. Since each instance of this class will give microphone signals of a certain size, from a specific source, and one environment (dataset):\n",
    "        05.01 I give in the constructor only one dataset name or choose the first matching one\n",
    "        05.02 I define the starting sample of the signal in the constructor, instead of hardcoding it whenever I use the \"get_mic\" function\n",
    "        05.03 I define the size of the signal in the constructor, instead of hardcoding it whenever I use the \"get_mic\" function\n",
    "    06. I can make a wrapper (torch.utils.data.Dataset) of this class to load only a set of microphones using the \"get_mic\" function.\n",
    "\"\"\"\n",
    "\n",
    "def read_npy_header(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        version = np.lib.format.read_magic(f)\n",
    "        shape, fortran_order, dtype = np.lib.format._read_array_header(f, version)\n",
    "    return shape, fortran_order, dtype\n",
    "\n",
    "class DB_microphones:\n",
    "    \"\"\"\n",
    "        Base class for microphone databases.\n",
    "        I define the @property methods here, so I don't have to redefine them in the subclasses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root: str):\n",
    "        self.root = root\n",
    "\n",
    "    @property\n",
    "    def raw_folder(self) -> str:\n",
    "        return os.path.join(self.root, self.__class__.__name__, \"raw\")\n",
    "\n",
    "    @property\n",
    "    def dt(self):\n",
    "        return 1.0 / self.fs    \n",
    "\n",
    "    @property\n",
    "    def fs(self):\n",
    "        return self._fs\n",
    "\n",
    "    @property\n",
    "    def n_mics(self):\n",
    "        return self._nmics\n",
    "    \n",
    "    @property\n",
    "    def nt(self):\n",
    "        return self._nt\n",
    "\n",
    "    @property\n",
    "    def n_sources(self):\n",
    "        return self._n_sources\n",
    "\n",
    "    @property\n",
    "    def source_id(self):\n",
    "        return self._source_id\n",
    "    \n",
    "    @property\n",
    "    def signal_size(self):\n",
    "        return self._signal_size\n",
    "    \n",
    "    @property\n",
    "    def start_signal(self):\n",
    "        return self._start_signal\n",
    "    \n",
    "    def get_nmics(self):\n",
    "        return self.n_mics\n",
    "\n",
    "    def get_mic(self, imic, start, size):\n",
    "        raise NotImplementedError(\"Must be implemented in subclass\")\n",
    "\n",
    "    def get_pos(self, imic):\n",
    "        raise NotImplementedError(\"Must be implemented in subclass\")\n",
    "\n",
    "    def get_time(self, start, size):\n",
    "        return (start + np.arange(size)) * self.dt\n",
    "\n",
    "\n",
    "class MeshRIR(DB_microphones):\n",
    "\n",
    "    mirrors = [\n",
    "        \"https://zenodo.org/records/10852693/files/\"\n",
    "    ]\n",
    "\n",
    "    resources = [\n",
    "        (\"S1-M3969_npy.zip\", \"2cb598eb44bb9905560c545db7af3432\" ),\n",
    "        (\"S32-M441_npy.zip\", \"9818fc66b36513590e7abd071243d8e9\"), \n",
    "    ]\n",
    "\n",
    "    _fs = 48000\n",
    "    \n",
    "\n",
    "    def __init__(self, root: str, \n",
    "                 download: bool = False, \n",
    "                 dataset: str = \"S1\", \n",
    "                 source_id: int = 0,\n",
    "                 start_signal: int = 0,\n",
    "                 signal_size: int = 512):\n",
    "        super().__init__(root=root)\n",
    "        self.root = root\n",
    "        self.dataset = dataset # Here it is user defined, later the dataset will save the name of the dataset according to the ones in self.resources\n",
    "        self._start_signal = start_signal\n",
    "        self._signal_size = signal_size\n",
    "\n",
    "        # check if /raw exists and load the data from there\n",
    "        # or download the data and process it\n",
    "        if download:\n",
    "            self.download(dataset=self.dataset)\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
    "        \n",
    "        # Before this, we could download multiple datasets, but we should only choose one\n",
    "        datasets = [filename for filename, _ in self.resources if dataset in filename]\n",
    "        if len(datasets) > 1:\n",
    "            print(f\"Warning, Datasets found for '{dataset}': \\n{datasets}. \\nUsing the first one: {datasets[0]}\")\n",
    "        self.dataset = Path(datasets[0]).stem\n",
    "\n",
    "        # Data path: \n",
    "        self.data_path = os.path.join(self.raw_folder, self.dataset)\n",
    "\n",
    "        # Check number of microphones (from positions and number of ir files)\n",
    "        pos_mics_shape, _, _ = read_npy_header(Path(self.data_path, \"pos_mic.npy\")) \n",
    "        self._nmics = pos_mics_shape[0]\n",
    "\n",
    "        # Source id\n",
    "        mic_signal_shape, _, _ = read_npy_header(Path(self.data_path, f\"ir_{0}.npy\"))\n",
    "        self._n_sources, self._nt = mic_signal_shape\n",
    "        assert source_id < self._n_sources , f\"Database has {self._n_sources} sources. Choose source_id in [0, {self._n_sources-1}]. \"\n",
    "        self._source_id = source_id\n",
    "        \n",
    "        # number of microphones. ir_ files in the folder, each is a microphone\n",
    "        nfiles = len(glob.glob(os.path.join(self.data_path, 'ir_*.npy')))\n",
    "        assert self._nmics==nfiles, f\"ir_xxx.npy files = {nfiles}, should be {self._nmics}\"\n",
    "\n",
    "    def load_src_positions(self):\n",
    "        # Source position in the dataset\n",
    "        pos_src_path = os.path.join(self.data_path, 'pos_src.npy')\n",
    "        self._source_positions = np.load(pos_src_path)\n",
    "\n",
    "    def load_mic_positions(self):\n",
    "        # Position of the microphones\n",
    "        pos_mic_path = os.path.join(self.data_path, 'pos_mic.npy')\n",
    "        self._pos_mics = np.load(pos_mic_path) # (nmics, 3)  each row is (x,y,z) for a mic\n",
    "\n",
    "    # Modified from MNIST, this one checks if the .zip files are int the /raw folder\n",
    "    def _check_exists(self, dataset:str = \"S1\") -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the numpy file exists in the raw folder.\n",
    "        If it exists, we do not download and extract.\n",
    "        It may be that the .zip file is there but not the numpy files,\n",
    "        in which case, download_and_extract_archive will be called.\n",
    "        If it detects the .zip if will just extract, otherwise download and extract.\n",
    "        dataset: str\n",
    "            The dataset to check for. It can be \"S1\" or \"S32\", if only one dataset to download\n",
    "            if \"S\" it downloads both datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # numpy files in the raw folder\n",
    "        files = glob.glob(os.path.join(self.raw_folder, \"*_npy\") )\n",
    "\n",
    "        urls = [url for url, _ in self.resources]\n",
    "\n",
    "        # Check if the pattern in dataset is in any of the resources\n",
    "        found_in_res = any(dataset in url for url in urls)\n",
    "        if not found_in_res:\n",
    "            raise ValueError(f\"Dataset '{dataset}' not found in any of the resources: {urls}\")\n",
    "\n",
    "        # Check if the pattern in dataset is in any of the numpy files\n",
    "        found = any(dataset in os.path.basename(file) for file in files)\n",
    "\n",
    "        return found\n",
    "\n",
    "    def download(self, dataset:str =\"S1\") -> None:\n",
    "        \"\"\"\n",
    "        Download the data if it doesn't exist already.\n",
    "        Args:\n",
    "            dataset (str): The dataset to download. It can be \"S1\" or \"S32\", if only one dataset to download\n",
    "            if \"S\" it downloads both datasets.\n",
    "        \"\"\"\n",
    "\n",
    "        # Checks that the .zip files exists\n",
    "        if self._check_exists(dataset):\n",
    "            return\n",
    "\n",
    "        # Create folders \n",
    "        os.makedirs(self.raw_folder, exist_ok=True)\n",
    "\n",
    "        # Check what file to download according to dataset\n",
    "        matching_resources = [(filename, md5) for filename, md5 in self.resources if dataset in filename]\n",
    "\n",
    "        # download files\n",
    "        for filename, md5 in matching_resources:\n",
    "            for mirror in self.mirrors:\n",
    "                \n",
    "                url = f\"{mirror}{filename}\"\n",
    "                try:\n",
    "                    download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5)\n",
    "                except URLError as error:\n",
    "                    print(f\"Failed to download (trying next):\\n{error}\")\n",
    "                    continue\n",
    "                finally:\n",
    "                    print()\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(f\"Error downloading {filename}\")\n",
    "    \n",
    "    def load_all_data(self):\n",
    "        # Concatenate vectors (source, signal) -> into -> (source, imic, signal)\n",
    "        data = np.concatenate( \n",
    "            [np.load(os.path.join(self.data_path, f'ir_{i}.npy'))[:,None,:]  \n",
    "             for i in range(self._nmics)], # for all mics\n",
    "             axis = 1 ) # in axis 1 (mics)  (source, mics, signal)\n",
    "        return data\n",
    "\n",
    "    def load_mic(self, imic):\n",
    "        mic_signal = np.load(os.path.join(self.data_path, f'ir_{imic}.npy')) # (source, signal)\n",
    "        return mic_signal[self.source_id, :]\n",
    "        \n",
    "    def get_pos(self, imic):\n",
    "        if not hasattr(self, \"_pos_mics\"):\n",
    "            self.load_mic_positions()\n",
    "        return self._pos_mics[imic,:]\n",
    "    \n",
    "    def get_mic(self, imic, start=None, size=None):\n",
    "        if start is None:\n",
    "            start = self.start_signal\n",
    "        if size is None:\n",
    "            size = self.signal_size\n",
    "        return self.load_mic(imic)[start:start+size]\n",
    "    \n",
    "    def get_time(self, start=None, size=None):\n",
    "        if start is None:\n",
    "            start = self.start_signal\n",
    "        if size is None:\n",
    "            size = self.signal_size\n",
    "        t0 = 0.0\n",
    "        t = np.arange(0, self._nt)*self.dt + t0\n",
    "        return t[start:start+size]\n",
    "    \n",
    "    def get_nmics(self):        \n",
    "        return self.n_mics\n",
    "\n",
    "    def get_src_pos(self):\n",
    "        if not hasattr(self, \"_source_positions\"):\n",
    "            self.load_src_positions()\n",
    "        return self._source_positions[self.source_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ds = MeshRIR(root=\"data\", download=True, dataset=\"S1\", source_id=0)\n",
    "print(ds.dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d79d5",
   "metadata": {},
   "source": [
    "Tests of class methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86159154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2. , 1.5, 0. ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds.load_all_data() # Pass\n",
    "ds.load_mic(3) # Pass\n",
    "ds.get_pos(1) # Pass\n",
    "ds.get_mic(3, start=0, size=ds.nt).shape # Pass\n",
    "ds.get_mic(3, start=0, size=ds.nt).shape # Pass\n",
    "ds.get_mic(3).shape # Pass\n",
    "ds.get_time(start=0, size=ds.nt).shape # Pass\n",
    "ds.get_time().shape # Pass\n",
    "ds.get_nmics() # Pass\n",
    "ds.get_src_pos() # Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234efc9",
   "metadata": {},
   "source": [
    "Test of property methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c13ecd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000\n",
      "2.0833333333333333e-05\n",
      "3969\n",
      "32768\n",
      "1\n",
      "0\n",
      "0\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "# ds.fs=4 # AttributeError: can't set attribute\n",
    "# ds.dt() # TypeError: 'float' object is not callable\n",
    "print(ds.fs)\n",
    "print(ds.dt)\n",
    "print(ds.n_mics)\n",
    "print(ds.nt)\n",
    "print(ds.n_sources)\n",
    "print(ds.source_id)\n",
    "print(ds.start_signal)\n",
    "print(ds.signal_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4bcf72",
   "metadata": {},
   "source": [
    "## Class for RIR Zea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1685328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d606eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ZeaRIR(DB_microphones):\n",
    "\n",
    "#     mirrors = [\n",
    "#         \"https://zenodo.org/records/10852693/files/\"\n",
    "#     ]\n",
    "\n",
    "#     resources = [\n",
    "#         (\"S1-M3969_npy.zip\", \"2cb598eb44bb9905560c545db7af3432\" ),\n",
    "#         (\"S32-M441_npy.zip\", \"9818fc66b36513590e7abd071243d8e9\"), \n",
    "#     ]\n",
    "\n",
    "#     _fs = 48000\n",
    "    \n",
    "\n",
    "#     def __init__(self, root: str, \n",
    "#                  download: bool = False, \n",
    "#                  dataset: str = \"S1\", \n",
    "#                  source_id: int = 0,\n",
    "#                  start_signal: int = 0,\n",
    "#                  signal_size: int = 512):\n",
    "#         super().__init__(root=root)\n",
    "#         self.root = root\n",
    "#         self.dataset = dataset # Here it is user defined, later the dataset will save the name of the dataset according to the ones in self.resources\n",
    "#         self._start_signal = start_signal\n",
    "#         self._signal_size = signal_size\n",
    "\n",
    "#         # check if /raw exists and load the data from there\n",
    "#         # or download the data and process it\n",
    "#         if download:\n",
    "#             self.download(dataset=self.dataset)\n",
    "\n",
    "#         if not self._check_exists():\n",
    "#             raise RuntimeError(\"Dataset not found. You can use download=True to download it\")\n",
    "        \n",
    "#         # Before this, we could download multiple datasets, but we should only choose one\n",
    "#         datasets = [filename for filename, _ in self.resources if dataset in filename]\n",
    "#         if len(datasets) > 1:\n",
    "#             print(f\"Warning, Datasets found for '{dataset}': \\n{datasets}. \\nUsing the first one: {datasets[0]}\")\n",
    "#         self.dataset = Path(datasets[0]).stem\n",
    "\n",
    "#         # Data path: \n",
    "#         self.data_path = os.path.join(self.raw_folder, self.dataset)\n",
    "\n",
    "#         # Check number of microphones (from positions and number of ir files)\n",
    "#         pos_mics_shape, _, _ = read_npy_header(Path(self.data_path, \"pos_mic.npy\")) \n",
    "#         self._nmics = pos_mics_shape[0]\n",
    "\n",
    "#         # Source id\n",
    "#         mic_signal_shape, _, _ = read_npy_header(Path(self.data_path, f\"ir_{0}.npy\"))\n",
    "#         self._n_sources, self._nt = mic_signal_shape\n",
    "#         assert source_id < self._n_sources , f\"Database has {self._n_sources} sources. Choose source_id in [0, {self._n_sources-1}]. \"\n",
    "#         self._source_id = source_id\n",
    "        \n",
    "#         # number of microphones. ir_ files in the folder, each is a microphone\n",
    "#         nfiles = len(glob.glob(os.path.join(self.data_path, 'ir_*.npy')))\n",
    "#         assert self._nmics==nfiles, f\"ir_xxx.npy files = {nfiles}, should be {self._nmics}\"\n",
    "\n",
    "#     def load_src_positions(self):\n",
    "#         # Source position in the dataset\n",
    "#         pos_src_path = os.path.join(self.data_path, 'pos_src.npy')\n",
    "#         self._source_positions = np.load(pos_src_path)\n",
    "\n",
    "#     def load_mic_positions(self):\n",
    "#         # Position of the microphones\n",
    "#         pos_mic_path = os.path.join(self.data_path, 'pos_mic.npy')\n",
    "#         self._pos_mics = np.load(pos_mic_path) # (nmics, 3)  each row is (x,y,z) for a mic\n",
    "\n",
    "#     # Modified from MNIST, this one checks if the .zip files are int the /raw folder\n",
    "#     def _check_exists(self, dataset:str = \"S1\") -> bool:\n",
    "#         \"\"\"\n",
    "#         Checks if the numpy file exists in the raw folder.\n",
    "#         If it exists, we do not download and extract.\n",
    "#         It may be that the .zip file is there but not the numpy files,\n",
    "#         in which case, download_and_extract_archive will be called.\n",
    "#         If it detects the .zip if will just extract, otherwise download and extract.\n",
    "#         dataset: str\n",
    "#             The dataset to check for. It can be \"S1\" or \"S32\", if only one dataset to download\n",
    "#             if \"S\" it downloads both datasets.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # numpy files in the raw folder\n",
    "#         files = glob.glob(os.path.join(self.raw_folder, \"*_npy\") )\n",
    "\n",
    "#         urls = [url for url, _ in self.resources]\n",
    "\n",
    "#         # Check if the pattern in dataset is in any of the resources\n",
    "#         found_in_res = any(dataset in url for url in urls)\n",
    "#         if not found_in_res:\n",
    "#             raise ValueError(f\"Dataset '{dataset}' not found in any of the resources: {urls}\")\n",
    "\n",
    "#         # Check if the pattern in dataset is in any of the numpy files\n",
    "#         found = any(dataset in os.path.basename(file) for file in files)\n",
    "\n",
    "#         return found\n",
    "\n",
    "#     def download(self, dataset:str =\"S1\") -> None:\n",
    "#         \"\"\"\n",
    "#         Download the data if it doesn't exist already.\n",
    "#         Args:\n",
    "#             dataset (str): The dataset to download. It can be \"S1\" or \"S32\", if only one dataset to download\n",
    "#             if \"S\" it downloads both datasets.\n",
    "#         \"\"\"\n",
    "\n",
    "#         # Checks that the .zip files exists\n",
    "#         if self._check_exists(dataset):\n",
    "#             return\n",
    "\n",
    "#         # Create folders \n",
    "#         os.makedirs(self.raw_folder, exist_ok=True)\n",
    "\n",
    "#         # Check what file to download according to dataset\n",
    "#         matching_resources = [(filename, md5) for filename, md5 in self.resources if dataset in filename]\n",
    "\n",
    "#         # download files\n",
    "#         for filename, md5 in matching_resources:\n",
    "#             for mirror in self.mirrors:\n",
    "                \n",
    "#                 url = f\"{mirror}{filename}\"\n",
    "#                 try:\n",
    "#                     download_and_extract_archive(url, download_root=self.raw_folder, filename=filename, md5=md5)\n",
    "#                 except URLError as error:\n",
    "#                     print(f\"Failed to download (trying next):\\n{error}\")\n",
    "#                     continue\n",
    "#                 finally:\n",
    "#                     print()\n",
    "#                 break\n",
    "#             else:\n",
    "#                 raise RuntimeError(f\"Error downloading {filename}\")\n",
    "    \n",
    "#     def load_all_data(self):\n",
    "#         # Concatenate vectors (source, signal) -> into -> (source, imic, signal)\n",
    "#         data = np.concatenate( \n",
    "#             [np.load(os.path.join(self.data_path, f'ir_{i}.npy'))[:,None,:]  \n",
    "#              for i in range(self._nmics)], # for all mics\n",
    "#              axis = 1 ) # in axis 1 (mics)  (source, mics, signal)\n",
    "#         return data\n",
    "\n",
    "#     def load_mic(self, imic):\n",
    "#         mic_signal = np.load(os.path.join(self.data_path, f'ir_{imic}.npy')) # (source, signal)\n",
    "#         return mic_signal[self.source_id, :]\n",
    "        \n",
    "#     def get_pos(self, imic):\n",
    "#         if not hasattr(self, \"_pos_mics\"):\n",
    "#             self.load_mic_positions()\n",
    "#         return self._pos_mics[imic,:]\n",
    "    \n",
    "#     def get_mic(self, imic, start=None, size=None):\n",
    "#         if start is None:\n",
    "#             start = self.start_signal\n",
    "#         if size is None:\n",
    "#             size = self.signal_size\n",
    "#         return self.load_mic(imic)[start:start+size]\n",
    "    \n",
    "#     def get_time(self, start=None, size=None):\n",
    "#         if start is None:\n",
    "#             start = self.start_signal\n",
    "#         if size is None:\n",
    "#             size = self.signal_size\n",
    "#         t0 = 0.0\n",
    "#         t = np.arange(0, self._nt)*self.dt + t0\n",
    "#         return t[start:start+size]\n",
    "    \n",
    "#     def get_nmics(self):        \n",
    "#         return self.n_mics\n",
    "\n",
    "#     def get_src_pos(self):\n",
    "#         if not hasattr(self, \"_source_positions\"):\n",
    "#             self.load_src_positions()\n",
    "#         return self._source_positions[self.source_id]\n",
    "\n",
    "# ds = MeshRIR(root=\"data\", download=True, dataset=\"S1\", source_id=0)\n",
    "# print(ds.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bcb33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FNOs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
